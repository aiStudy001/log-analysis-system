# Log Analysis System - Integration Test Guide

## ğŸ“‹ Quick Start

```bash
# 1. Start all services
docker-compose up -d

# 2. Check service health
curl http://localhost:8001/health  # Backend
curl http://localhost:3000         # Frontend
curl http://localhost:8000/docs    # Log-save server

# 3. Verify database
docker exec -it log-analysis-postgres psql -U loguser -d logdb -c "SELECT COUNT(*) FROM logs;"

# 4. Generate test data
python scripts/generate_test_logs.py --scenario all

# 5. Open frontend
open http://localhost:3000
```

---

## ğŸ§ª Feature #1: Query Result Cache

### Test 1.1: Cache Miss â†’ Cache Hit

**Purpose**: Verify caching mechanism improves query performance

**Steps**:
1. Open frontend at http://localhost:3000
2. Enter question: `ìµœê·¼ 1ì‹œê°„ ì—ëŸ¬ ë¡œê·¸`
3. Observe first execution:
   - Response time: ~4-5 seconds
   - Watch WebSocket events in browser DevTools
   - Verify `complete` event has `cache_hit: false`
4. Enter SAME question again immediately
5. Observe second execution:
   - Response time: <100ms (instant)
   - Verify `complete` event has `cache_hit: true`
6. Check UI for cache indicator (âš¡ badge)

**Expected Results**:
- âœ… First query: cache_hit=false, normal execution
- âœ… Second query: cache_hit=true, <100ms response
- âœ… Cache hit badge visible in UI
- âœ… No re-execution of SQL or Claude API calls

**Verification Commands**:
```bash
# Monitor backend logs during test
docker-compose logs -f log-analysis-server | grep -i cache

# Check if cache service is working (look for "Cache HIT" vs "Cache MISS" logs)
```

---

### Test 1.2: Cache Invalidation

**Purpose**: Ensure cache clears when new data inserted

**Steps**:
1. Execute a query: `ìµœê·¼ payment-api ì—ëŸ¬`
2. Verify it gets cached (second execution is instant)
3. Insert new logs via API:
   ```bash
   curl -X POST http://localhost:8000/logs \
     -H "Content-Type: application/json" \
     -d '{"logs":[{"timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%S)'","level":"ERROR","service":"payment-api","message":"Cache invalidation test"}]}'
   ```
4. Call cache invalidation endpoint:
   ```bash
   curl -X POST http://localhost:8001/invalidate_cache
   ```
5. Execute the same query again
6. Verify cache_hit=false (cache was cleared)

**Expected Results**:
- âœ… After invalidation, query re-executes fully
- âœ… New logs are included in results
- âœ… Response time returns to normal (~4-5s)

---

### Test 1.3: TTL Expiration

**Purpose**: Verify automatic cache expiration after TTL period

**Steps**:
1. Check current TTL setting:
   ```bash
   docker exec log-analysis-server grep CACHE_TTL .env
   # Default: CACHE_TTL_SECONDS=300 (5 minutes)
   ```
2. Execute a query and let it cache
3. **Option A**: Wait 5+ minutes
4. **Option B**: Reduce TTL for faster testing:
   ```bash
   # Edit .env to set CACHE_TTL_SECONDS=60
   # Restart: docker-compose restart log-analysis-server
   # Wait 1+ minute
   ```
5. Execute same query again
6. Verify cache_hit=false (TTL expired)

**Expected Results**:
- âœ… After TTL expires, cache automatically invalidates
- âœ… Query re-executes without manual intervention

---

## ğŸ§  Feature #2: Context-Aware Agent

### Test 2.1: Focus Tracking

**Purpose**: Verify focus entity extraction and UI display

**Steps**:
1. Ask: `ìµœê·¼ payment-api ì—ëŸ¬ëŠ”?`
2. Watch for `context_resolved` WebSocket event
3. Check event data:
   ```json
   {
     "type": "context_resolved",
     "node": "resolve_context",
     "data": {
       "resolution_needed": false,
       "original_question": "ìµœê·¼ payment-api ì—ëŸ¬ëŠ”?",
       "focus": {
         "service": "payment-api"
       }
     }
   }
   ```
4. Verify ConversationContext component appears
5. Check focus badge shows: "Service: payment-api"

**Expected Results**:
- âœ… Focus extracted from SQL query
- âœ… ConversationContext component visible
- âœ… Service badge displayed correctly

---

### Test 2.2: Reference Resolution

**Purpose**: Verify pronoun/reference resolution using Claude

**Steps**:
1. **Turn 1**: Ask `ìµœê·¼ payment-api ì—ëŸ¬ëŠ”?`
   - Verify focus set: `{service: "payment-api"}`

2. **Turn 2**: Ask `ê·¸ ì„œë¹„ìŠ¤ì˜ ëŠë¦° APIëŠ”?` (contains reference "ê·¸ ì„œë¹„ìŠ¤")

3. Observe `context_resolved` event:
   ```json
   {
     "type": "context_resolved",
     "data": {
       "resolution_needed": true,
       "original_question": "ê·¸ ì„œë¹„ìŠ¤ì˜ ëŠë¦° APIëŠ”?",
       "resolved_question": "payment-api ì„œë¹„ìŠ¤ì˜ ëŠë¦° APIëŠ”?",
       "focus": {
         "service": "payment-api"
       }
     }
   }
   ```

4. Check generated SQL includes:
   ```sql
   WHERE service = 'payment-api'
   ```

**Expected Results**:
- âœ… Reference "ê·¸ ì„œë¹„ìŠ¤" correctly resolved to "payment-api"
- âœ… Resolved question displayed (if UI shows it)
- âœ… SQL query includes correct service filter
- âœ… Context-aware status message in UI

---

### Test 2.3: Conversation History & Context Propagation

**Purpose**: Verify multi-turn context memory

**Steps**:
1. **Turn 1**: `DatabaseConnectionError ì—ëŸ¬ ë³´ì—¬ì¤˜`
   - Check focus: `{error_type: "DatabaseConnectionError"}`

2. **Turn 2**: `ê·¸ ì—ëŸ¬ê°€ ì–¸ì œë¶€í„° ì‹œì‘ëì–´?`
   - Verify resolved includes error_type
   - SQL should have: `WHERE error_type = 'DatabaseConnectionError'`

3. **Turn 3**: `ê·¸ ì—ëŸ¬ì˜ ê°œìˆ˜ëŠ”?`
   - Context should still maintain error_type
   - SQL aggregation on DatabaseConnectionError

**Expected Results**:
- âœ… Error type context maintained across 3 turns
- âœ… Each follow-up query correctly references previous focus
- âœ… No need to repeat "DatabaseConnectionError" in questions

---

### Test 2.4: New Conversation Reset

**Purpose**: Verify conversation reset clears all context

**Steps**:
1. Execute 3-4 queries to build comprehensive focus:
   - "payment-api ì—ëŸ¬"
   - "DatabaseConnectionError íƒ€ì…"
   - "ìµœê·¼ 1ì‹œê°„"
2. Verify ConversationContext shows multiple badges
3. Click **"ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘"** button
4. Verify:
   - ConversationContext component disappears or shows empty
   - conversation_id changes (check in browser DevTools)
   - chatStore.currentFocus = {} (empty)
5. Ask a new question - should have no previous context

**Expected Results**:
- âœ… All focus badges removed
- âœ… Fresh conversation_id generated
- âœ… No context from previous session
- âœ… Clean slate for new conversation

---

## ğŸ”€ Feature #3: Multi-Step Reasoning

### Test 3.1: Simple Query â†’ Single Step

**Purpose**: Verify simple queries bypass multi-step decomposition

**Steps**:
1. Ask simple question: `ìµœê·¼ 1ì‹œê°„ ì—ëŸ¬ ë¡œê·¸`
2. Watch for `plan_generated` event:
   ```json
   {
     "type": "plan_generated",
     "data": {
       "step_count": 1,
       "is_multi_step": false
     }
   }
   ```
3. Verify MultiStepProgress component does NOT appear
4. Query executes via standard workflow:
   - generate_sql â†’ validate_sql â†’ execute_query â†’ generate_insight

**Expected Results**:
- âœ… No decomposition for simple queries
- âœ… Original 5-node workflow used
- âœ… Faster execution (no planning overhead)

---

### Test 3.2: Complex Query â†’ Multi-Step Decomposition

**Purpose**: Verify complex analytical questions decompose into steps

**Steps**:
1. Ask complex question: `ê²°ì œ ì‹¤íŒ¨ìœ¨ì´ ì™œ ë†’ì•„ì¡Œì–´?`

2. Watch for `plan_generated` event:
   ```json
   {
     "type": "plan_generated",
     "data": {
       "step_count": 4,
       "is_multi_step": true,
       "steps": [
         {"index": 0, "description": "Calculate current payment failure rate"},
         {"index": 1, "description": "Compare with historical baseline"},
         {"index": 2, "description": "Identify concurrent errors"},
         {"index": 3, "description": "Analyze affected user patterns"}
       ],
       "synthesis": "Compare rates and identify root cause"
     }
   }
   ```

3. Verify MultiStepProgress component displays

4. Observe sequential `step_completed` events:
   ```json
   {
     "type": "step_completed",
     "data": {
       "step_index": 0,
       "step_count": 4,
       "description": "Calculate current payment failure rate",
       "question": "ìµœê·¼ 1ì‹œê°„ payment ê´€ë ¨ ì—ëŸ¬ ê°œìˆ˜ì™€ ì „ì²´ payment ë¡œê·¸ ê°œìˆ˜",
       "sql": "SELECT ...",
       "result_count": 25,
       "execution_time_ms": 156
     }
   }
   ```

5. Watch each step execute in sequence (0 â†’ 1 â†’ 2 â†’ 3)

6. Final `all_steps_complete` event:
   ```json
   {
     "type": "all_steps_complete",
     "data": {
       "total_steps": 4
     }
   }
   ```

7. Final insight combines all step results

**Expected Results**:
- âœ… Question decomposes into 3-5 logical steps
- âœ… Each step has clear description and purpose
- âœ… Steps execute sequentially (not parallel)
- âœ… Each step's results inform next step
- âœ… Final insight synthesizes all findings

**Example Final Insight**:
```
"ë¶„ì„ ê²°ê³¼, ê²°ì œ ì‹¤íŒ¨ìœ¨ì´ 20%ì—ì„œ 45%ë¡œ ê¸‰ì¦í–ˆìŠµë‹ˆë‹¤ (2ë°° ì´ìƒ ì¦ê°€).
ì£¼ìš” ì›ì¸ì€ DatabaseConnectionErrorë¡œ, ìµœê·¼ 1ì‹œê°„ ë™ì•ˆ ì§‘ì¤‘ì ìœ¼ë¡œ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
ì˜í–¥ë°›ì€ ì‚¬ìš©ìëŠ” ì£¼ë¡œ user_1~user_20 ë²”ìœ„ì´ë©°, /api/v1/checkout ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ë°œìƒí•©ë‹ˆë‹¤.
ì¦‰ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì„¤ì •ì„ í™•ì¸í•˜ê³  connection timeoutì„ ì¡°ì •í•˜ì„¸ìš”."
```

---

### Test 3.3: Step Progress Visualization

**Purpose**: Verify real-time UI updates during multi-step execution

**Steps**:
1. Execute complex query (triggers multi-step)
2. Observe MultiStepProgress component in real-time:

**Initial State** (after plan_generated):
```
Step 1: [â³ pending] Calculate current failure rate
Step 2: [â³ pending] Compare with baseline
Step 3: [â³ pending] Identify concurrent errors
Step 4: [â³ pending] Analyze user patterns

Progress: [â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0%
```

**During Execution** (step 0 active):
```
Step 1: [ğŸ”„ active] Calculate current failure rate
Step 2: [â³ pending] Compare with baseline
...

Progress: [â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 25%
```

**After Step 1 Completes**:
```
Step 1: [âœ… completed] Calculate current failure rate
        SQL: SELECT COUNT(*) ...
        25 rows, 156ms
Step 2: [ğŸ”„ active] Compare with baseline
...

Progress: [â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘] 50%
```

3. Check UI elements:
   - Step cards have correct border colors (gray â†’ blue â†’ green)
   - Status icons update (â³ â†’ ğŸ”„ â†’ âœ…)
   - SQL code expandable in completed steps
   - Result count and execution time displayed
   - Progress bar animates smoothly

**Expected Results**:
- âœ… Real-time status updates for each step
- âœ… Clear visual differentiation (pending/active/completed)
- âœ… Progress bar reflects completion percentage
- âœ… Detailed metrics per step
- âœ… Smooth animations and transitions

---

### Test 3.4: Step Failure Handling

**Purpose**: Verify graceful error handling in multi-step workflow

**Steps**:
1. Ask intentionally problematic question that will fail mid-execution:
   - Example: `ì¡´ì¬í•˜ì§€_ì•ŠëŠ”_ì»¬ëŸ¼ìœ¼ë¡œ í•„í„°ë§í•´ì„œ ê²°ì œ ë¶„ì„í•´ì¤˜`
   - This should cause SQL validation or execution error in one of the steps

2. Observe step execution until failure occurs

3. Watch for `step_failed` event:
   ```json
   {
     "type": "step_failed",
     "node": "execute_step",
     "status": "failed",
     "data": {
       "step_index": 1,
       "step_count": 3,
       "description": "Filter by invalid column",
       "error": "column \"ì¡´ì¬í•˜ì§€_ì•ŠëŠ”_ì»¬ëŸ¼\" does not exist"
     }
   }
   ```

4. Check UI behavior:
   - Failed step: red border + âŒ icon
   - Error message displayed clearly
   - Subsequent steps remain pending (NOT executed)
   - Overall error message shown to user

**Expected Results**:
- âœ… Step failure stops execution immediately
- âœ… Error clearly indicated in UI (red styling)
- âœ… Error message visible and understandable
- âœ… No execution of remaining steps
- âœ… User can retry with corrected question

---

## âš™ï¸ Feature #4: Query Planning & Optimization

### Test 4.1: Complexity Classification

**Purpose**: Verify accurate query complexity analysis

**Test Cases**:

**Case A - Simple Query**:
```
Question: "ìµœê·¼ ì—ëŸ¬ ë¡œê·¸"
Expected: complexity=simple
```

**Case B - Moderate Query**:
```
Question: "ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬ í†µê³„"
Expected: complexity=moderate
Reason: GROUP BY aggregation
```

**Case C - Complex Query**:
```
Question: "ì™œ ì—ëŸ¬ìœ¨ì´ ë†’ì•„ì¡Œì–´?"
Expected: complexity=complex
Reason: Root cause analysis keyword ("ì™œ")
```

**Steps**:
1. Execute each test case
2. Watch for `optimization_complete` event:
   ```json
   {
     "type": "optimization_complete",
     "node": "optimize_query",
     "data": {
       "complexity": "simple",
       "strategy": "single_query",
       "indexes": ["idx_service_level_time"]
     }
   }
   ```
3. Verify complexity matches expected

**Expected Results**:
- âœ… Simple queries: complexity=simple
- âœ… Aggregation queries: complexity=moderate
- âœ… Analytical queries: complexity=complex
- âœ… optimization_complete event emitted

---

### Test 4.2: Optimization Strategy Selection

**Purpose**: Verify appropriate strategy for each complexity level

**Test Matrix**:

| Query | Complexity | Expected Strategy | Workflow |
|-------|-----------|------------------|----------|
| "ìµœê·¼ ì—ëŸ¬" | simple | single_query | Standard SQL path |
| "ì„œë¹„ìŠ¤ë³„ í†µê³„" | moderate | single_query | Standard SQL path |
| "ì™œ ì‹¤íŒ¨ìœ¨ì´ ë†’ì•„?" | complex | use_multi_step | Multi-step path |

**Steps**:
1. Execute each query type
2. Verify strategy in `optimization_complete` event
3. Confirm workflow routing:
   - single_query â†’ generate_sql node
   - use_multi_step â†’ execute_step node

**Expected Results**:
- âœ… Correct strategy for each complexity
- âœ… Complex queries routed to multi-step
- âœ… Simple queries use efficient single-step path

---

### Test 4.3: Index Suggestions

**Purpose**: Verify relevant index recommendations based on query patterns

**Test Cases**:

**Case A - Service Filter**:
```
Question: "serviceë³„ ì—ëŸ¬ ë¶„í¬"
Expected indexes: ["idx_service_level_time"]
```

**Case B - User Filter**:
```
Question: "userë³„ ì—ëŸ¬ íŒ¨í„´"
Expected indexes: ["idx_user_time"]
```

**Case C - Error Type Filter**:
```
Question: "error_typeë³„ í†µê³„"
Expected indexes: ["idx_error_time"]
```

**Steps**:
1. Execute each query
2. Check `optimization_complete` event data.indexes
3. Verify suggested indexes are relevant to query filters

**Expected Results**:
- âœ… Correct index suggestions based on WHERE clauses
- âœ… Suggestions appear in event data
- âœ… (Optional) UI displays optimization hints

---

## ğŸš¨ Feature #5: Alerting & Monitoring

### Test 5.1: Manual Anomaly Check API

**Purpose**: Verify on-demand anomaly detection endpoint

**Steps**:
```bash
# Trigger manual check
curl -X POST http://localhost:8001/alerts/check

# Expected response
{
  "alerts": [],
  "count": 0
}
# Or if anomalies exist:
{
  "alerts": [
    {
      "type": "error_rate_spike",
      "severity": "warning",
      "message": "ì—ëŸ¬ìœ¨ 25.0% ì¦ê°€ ê°ì§€ (ìµœê·¼ 5ë¶„)",
      "data": {
        "current_count": 50,
        "baseline_count": 40,
        "spike_percentage": 25.0
      }
    }
  ],
  "count": 1
}
```

**Expected Results**:
- âœ… Endpoint returns current anomaly status
- âœ… Response includes all 3 check types (error_rate, slow_api, service_down)
- âœ… Returns empty array if no anomalies

---

### Test 5.2: Alert History API

**Purpose**: Verify alert history retrieval

**Steps**:
```bash
# Get recent alerts
curl http://localhost:8001/alerts/history?limit=20

# Expected response
{
  "alerts": [
    {
      "type": "error_rate_spike",
      "severity": "critical",
      "message": "ì—ëŸ¬ìœ¨ 50.0% ì¦ê°€ ê°ì§€",
      "data": {...},
      "timestamp": "2026-02-05T10:30:00"
    }
  ]
}
```

**Expected Results**:
- âœ… Returns historical alerts (may be empty initially)
- âœ… Each alert has: type, severity, message, data, timestamp
- âœ… Limit parameter works correctly

---

### Test 5.3: Error Rate Spike Detection

**Purpose**: Verify automatic spike detection via background task

**Steps**:

**1. Establish Baseline** (5 minutes of normal activity):
```bash
python scripts/generate_test_logs.py --scenario normal --count 50
# Wait 5 minutes for baseline period
```

**2. Generate Error Spike**:
```bash
# Generate 100 ERROR logs in ~5 seconds
for i in {1..100}; do
  curl -X POST http://localhost:8000/logs \
    -H "Content-Type: application/json" \
    -d '{"logs":[{"timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%S)'","level":"ERROR","service":"payment-api","message":"Spike test '$i'","error_type":"TestSpike"}]}'
  sleep 0.05
done
```

**3. Wait for Background Task**:
- Background task runs every 5 minutes
- Maximum detection delay: 5 minutes
- Alert should trigger within 6 minutes

**4. Verify Alert**:
- Check frontend: Toast notification should appear
- Check API:
  ```bash
  curl http://localhost:8001/alerts/history?limit=1
  ```
- Verify alert data:
  ```json
  {
    "type": "error_rate_spike",
    "severity": "critical",
    "message": "ì—ëŸ¬ìœ¨ X% ì¦ê°€ ê°ì§€ (ìµœê·¼ 5ë¶„)",
    "data": {
      "current_count": 100,
      "baseline_count": ~10,
      "spike_percentage": ~900
    }
  }
  ```

**Expected Results**:
- âœ… Alert detected within 6 minutes
- âœ… Spike percentage calculated correctly
- âœ… Severity = critical (spike >50%)
- âœ… WebSocket broadcast to frontend
- âœ… Toast notification displays

---

### Test 5.4: Slow API Detection

**Purpose**: Verify slow API identification (>2 seconds)

**Steps**:

**1. Generate Slow API Logs**:
```bash
# Generate 10 slow API requests
for i in {1..10}; do
  curl -X POST http://localhost:8000/logs \
    -H "Content-Type: application/json" \
    -d '{"logs":[{"timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%S)'","level":"INFO","service":"payment-api","path":"/api/v1/checkout","duration_ms":3500,"message":"Slow request '$i'"}]}'
  sleep 0.5
done
```

**2. Wait 5 Minutes**:
- Background task will analyze last 10 minutes
- Looks for APIs with avg duration_ms > 2000

**3. Verify Alert**:
```bash
curl http://localhost:8001/alerts/history?limit=1
```

Expected:
```json
{
  "type": "slow_api",
  "severity": "warning",
  "message": "1ê°œ ëŠë¦° API ê°ì§€ (>2ì´ˆ)",
  "data": {
    "slow_apis": [
      {
        "path": "/api/v1/checkout",
        "service": "payment-api",
        "avg_duration": 3500,
        "count": 10
      }
    ]
  }
}
```

**Expected Results**:
- âœ… Slow APIs detected (>2000ms threshold)
- âœ… Average duration calculated correctly
- âœ… Alert includes API path and service
- âœ… Count shows number of slow requests

---

### Test 5.5: Service Down Detection

**Purpose**: Verify detection of services without logs for 5+ minutes

**Steps**:

**1. Establish Service Activity**:
```bash
# Generate logs for "test-service"
for i in {1..20}; do
  curl -X POST http://localhost:8000/logs \
    -H "Content-Type: application/json" \
    -d '{"logs":[{"timestamp":"'$(date -u +%Y-%m-%dT%H:%M:%S)'","level":"INFO","service":"test-service","message":"Active '$i'"}]}'
  sleep 2
done
```

**2. Stop Generating Logs**:
- Simply stop - don't generate any more logs for "test-service"
- Other services can continue generating logs

**3. Wait 6+ Minutes**:
- Background task checks for services with no logs in last 5 minutes

**4. Verify Alert**:
```bash
curl http://localhost:8001/alerts/history?limit=1
```

Expected:
```json
{
  "type": "service_down",
  "severity": "critical",
  "message": "1ê°œ ì„œë¹„ìŠ¤ ë¡œê·¸ ì—†ìŒ (5ë¶„)",
  "data": {
    "services": ["test-service"]
  }
}
```

**Expected Results**:
- âœ… Service flagged as down after 5 min silence
- âœ… Severity = critical (service unavailability is critical)
- âœ… Alert includes list of down services
- âœ… Only silent services flagged (others continue normally)

---

### Test 5.6: Alert Toast Notification UI

**Purpose**: Verify real-time alert UI and user interaction

**Steps**:
1. Trigger any alert (error spike recommended for speed)
2. **Immediately after alert triggers**, observe frontend UI:

**Alert Appearance**:
- Toast appears bottom-right corner
- Severity icon matches level:
  - â„¹ï¸ for info
  - âš ï¸ for warning
  - ğŸš¨ for critical
- Border color matches severity:
  - Blue for info
  - Yellow for warning
  - Red for critical

**Alert Content**:
- Title: Alert type (e.g., "Error Rate Spike")
- Message: Clear description
- Details: Expandable JSON data
- Timestamp: Localized Korean format

**Interaction**:
3. **Auto-Dismiss Test**: Wait 10 seconds
   - Toast should auto-hide

4. **Manual Dismiss Test**: Trigger another alert
   - Click âœ• button
   - Toast should immediately close

5. **Multiple Alerts**: Trigger 2-3 alerts rapidly
   - Only latest alert shown
   - Previous alerts accessible in history

**Expected Results**:
- âœ… Toast appears within 1-2 seconds of alert
- âœ… Correct severity styling (icon + border)
- âœ… Message clear and actionable
- âœ… Details expandable
- âœ… Auto-dismiss after 10s works
- âœ… Manual dismiss works instantly
- âœ… No overlapping toasts

---

### Test 5.7: Alert History Page

**Purpose**: Verify alert history UI and filtering

**Steps**:
1. Generate several alerts (different types and severities)
2. Navigate to `/alerts` route in frontend
3. Verify alert list displays all historical alerts

**UI Elements to Check**:
- Alert cards show: icon, title, severity badge, message, timestamp
- Data expandable in details
- Sorted by timestamp (newest first)

**Test Filtering**:
4. Use severity filter dropdown:
   - Select "Warning" â†’ only warning alerts shown
   - Select "Critical" â†’ only critical alerts shown
   - Select "All" â†’ all alerts shown

**Test Clear Functionality**:
5. Click "Clear All" button
6. Verify all alerts removed from history
7. Empty state message: "No alerts"

**Test Unread Count**:
8. Generate new alert
9. Check Sidebar: unread count badge appears
10. Navigate to /alerts
11. Verify unread count clears (markAllAsRead on mount)

**Expected Results**:
- âœ… All historical alerts visible
- âœ… Filtering by severity works
- âœ… Clear All removes all alerts
- âœ… Unread count in sidebar
- âœ… Auto-mark-as-read when viewing history
- âœ… Empty state handled gracefully

---

## ğŸ”§ Feature #6: Tool Selection Layer

### Test 6.1: SQL Tool Selection (Default)

**Purpose**: Verify SQL is default tool for standard queries

**Steps**:
1. Ask standard question: `ìµœê·¼ ì—ëŸ¬ ë¡œê·¸`
2. Watch for `tool_selected` event:
   ```json
   {
     "type": "tool_selected",
     "node": "tool_selector",
     "data": {
       "tool": "sql",
       "reason": "Best for structured queries and data filtering"
     }
   }
   ```
3. Verify standard SQL workflow executes

**Expected Results**:
- âœ… SQL tool selected by default
- âœ… tool_selected event emitted
- âœ… Normal SQL execution path

---

### Test 6.2: Grep Tool Selection for Patterns

**Purpose**: Verify pattern matching queries route to grep tool

**Steps**:
1. Ask pattern question: `'timeout' íŒ¨í„´ í¬í•¨ëœ ë¡œê·¸`
2. Watch for `tool_selected` event:
   ```json
   {
     "type": "tool_selected",
     "data": {
       "tool": "grep",
       "reason": "Best for pattern matching and text search"
     }
   }
   ```
3. Verify SQL uses LIKE pattern:
   ```sql
   SELECT * FROM logs
   WHERE message LIKE '%timeout%'
   AND deleted = FALSE
   ORDER BY created_at DESC
   ```
4. Check results contain "timeout" in message field

**Expected Results**:
- âœ… Grep tool selected for pattern queries
- âœ… Pattern extracted from question correctly
- âœ… SQL uses LIKE operator
- âœ… Results match pattern

**Pattern Detection Keywords**:
- 'íŒ¨í„´', 'ìœ ì‚¬í•œ', 'matching', 'í¬í•¨ëœ', 'contains', 'search'

---

### Test 6.3: Metrics Tool Selection & Fallback

**Purpose**: Verify metrics tool detection and graceful fallback

**Steps**:
1. Ask metrics question: `ì „ì²´ ì„œë¹„ìŠ¤ í†µê³„`
2. Watch for `tool_selected` event:
   ```json
   {
     "type": "tool_selected",
     "data": {
       "tool": "sql",
       "reason": "Best for structured queries and data filtering"
     }
   }
   ```
3. Verify it falls back to SQL (metrics API not yet implemented)
4. SQL should still provide aggregated statistics

**Expected Results**:
- âœ… Tool selection logic identifies metrics keywords
- âœ… Graceful fallback to SQL
- âœ… Query still succeeds and returns stats
- âœ… No errors due to missing metrics tool

**Metrics Keywords**:
- 'ì „ì²´', 'í†µê³„', 'summary', 'ê°œìš”', 'overview', 'dashboard'

---

## ğŸ”— Integration Test Scenarios

### Scenario A: Cache + Context + Multi-Step

**Purpose**: Test multiple features working together in realistic workflow

**Complete Conversation Flow**:

**Turn 1: Initial Query (Set Context)**
```
User: "ìµœê·¼ payment-api ì—ëŸ¬ëŠ”?"
System:
  - context_resolved: no references, focus set to {service: payment-api}
  - optimization_complete: complexity=simple
  - plan_generated: is_multi_step=false (simple query)
  - Standard SQL execution
  - complete: cache_hit=false, results displayed
  - Focus badge appears: "Service: payment-api"
```

**Turn 2: Same Query (Cache Hit)**
```
User: "ìµœê·¼ payment-api ì—ëŸ¬ëŠ”?"
System:
  - cache_hit event immediately
  - complete: cache_hit=true, <100ms response
  - âš¡ Cached badge displayed
  - No SQL execution, no Claude API calls
```

**Turn 3: Context Reference (Cache Miss, New Query)**
```
User: "ê·¸ ì„œë¹„ìŠ¤ì˜ ëŠë¦° APIëŠ”?"
System:
  - context_resolved: resolution_needed=true
  - Original: "ê·¸ ì„œë¹„ìŠ¤ì˜ ëŠë¦° APIëŠ”?"
  - Resolved: "payment-api ì„œë¹„ìŠ¤ì˜ ëŠë¦° APIëŠ”?"
  - optimization_complete: complexity=simple
  - SQL includes: WHERE service = 'payment-api' AND duration_ms > 1000
  - complete: cache_hit=false (new question)
  - New cache entry created
```

**Turn 4: Complex Follow-up (Multi-Step)**
```
User: "ì™œ ëŠë ¤ì¡Œì–´?"
System:
  - context_resolved: "payment-apiê°€ ì™œ ëŠë ¤ì¡Œì–´?"
  - optimization_complete: complexity=complex, strategy=use_multi_step
  - plan_generated: is_multi_step=true, step_count=3
    - Step 0: Current slow API count
    - Step 1: Historical slow API baseline
    - Step 2: Concurrent system issues
  - MultiStepProgress displayed
  - step_completed events (0, 1, 2)
  - all_steps_complete
  - Final insight synthesizes: "payment-apiì˜ /checkout ì—”ë“œí¬ì¸íŠ¸ê°€
    í‰ê·  3.2ì´ˆë¡œ ëŠë ¤ì¡ŒìŠµë‹ˆë‹¤ (ê¸°ì¡´ 500ms). ë™ì‹œì— DatabaseConnectionErrorê°€
    ë°œìƒí•˜ì—¬ ì—°ê²° ëŒ€ê¸° ì‹œê°„ì´ ì¦ê°€í•œ ê²ƒì´ ì›ì¸ì…ë‹ˆë‹¤."
```

**Verification Checklist**:
- âœ… Cache reduces redundant queries (Turn 2)
- âœ… Context maintained across all turns
- âœ… References resolved correctly (Turn 3)
- âœ… Complex questions trigger multi-step (Turn 4)
- âœ… All features work seamlessly together
- âœ… Focus badges update correctly
- âœ… No conflicts or errors

---

### Scenario B: Alert-Driven Conversation

**Purpose**: Test alert system integration with context-aware chat

**Flow**:

**Step 1: Generate Alert Condition**
```bash
# Create error spike
python scripts/generate_test_logs.py --scenario error_spike --count 100
```

**Step 2: Wait for Alert** (~5 minutes)
- Background task detects spike
- Alert broadcast via WebSocket
- Toast notification appears:
  ```
  ğŸš¨ Error Rate Spike
  ì—ëŸ¬ìœ¨ 150.0% ì¦ê°€ ê°ì§€ (ìµœê·¼ 5ë¶„)

  Details:
  {
    "current_count": 100,
    "baseline_count": 40,
    "spike_percentage": 150.0
  }
  ```

**Step 3: Context-Aware Follow-up**
```
User: "ë°©ê¸ˆ alertì— ëŒ€í•´ ìì„¸íˆ ì•Œë ¤ì¤˜"
System:
  - context_resolved: Uses alert data from focus
  - If alert had service info, includes in query
  - Retrieves detailed error logs
  - Analyzes spike timing, error types, affected users
  - Provides actionable recommendations
```

**Step 4: Deep Dive**
```
User: "ê·¸ ì—ëŸ¬ì˜ ì›ì¸ì€?"
System:
  - Context maintains error_type from alert
  - Multi-step analysis:
    - Step 0: Error distribution by time
    - Step 1: Concurrent system events
    - Step 2: Affected endpoints
  - Root cause analysis insight
```

**Verification Checklist**:
- âœ… Alert generated and broadcast
- âœ… Toast displays alert details
- âœ… User can reference "ë°©ê¸ˆ alert"
- âœ… Context-aware analysis uses alert data
- âœ… Multi-step triggered for root cause
- âœ… Comprehensive actionable insights

---

### Scenario C: Complex Analysis Pipeline

**Purpose**: Test full advanced feature stack in production workflow

**Question**: `ì–´ë–¤ ì„œë¹„ìŠ¤ê°€ ê°€ì¥ ë¬¸ì œì¸ê°€?`

**Complete Pipeline Execution**:

**1. Optimization Analysis**:
```json
{
  "type": "optimization_complete",
  "data": {
    "complexity": "complex",
    "strategy": "use_multi_step",
    "indexes": ["idx_service_level_time"]
  }
}
```

**2. Query Planning**:
```json
{
  "type": "plan_generated",
  "data": {
    "step_count": 4,
    "is_multi_step": true,
    "steps": [
      {"index": 0, "description": "Count errors by service (24h)"},
      {"index": 1, "description": "Top 3 services error type distribution"},
      {"index": 2, "description": "Severity analysis per service"},
      {"index": 3, "description": "Trend analysis (today vs yesterday)"}
    ]
  }
}
```

**3. Multi-Step Execution**:

**Step 0**: Count errors by service
```sql
SELECT service, COUNT(*) as error_count
FROM logs
WHERE level = 'ERROR'
  AND created_at > NOW() - INTERVAL '24 hours'
  AND deleted = FALSE
GROUP BY service
ORDER BY error_count DESC
```
Result: payment-api (250), user-api (180), order-api (120)

**Step 1**: Error type distribution for top 3
```sql
SELECT service, error_type, COUNT(*) as count
FROM logs
WHERE level = 'ERROR'
  AND service IN ('payment-api', 'user-api', 'order-api')
  AND created_at > NOW() - INTERVAL '24 hours'
GROUP BY service, error_type
ORDER BY service, count DESC
```

**Step 2**: Severity analysis
```sql
SELECT service, level, COUNT(*) as count
FROM logs
WHERE service IN ('payment-api', 'user-api', 'order-api')
  AND created_at > NOW() - INTERVAL '24 hours'
GROUP BY service, level
```

**Step 3**: Trend comparison
```sql
-- Today vs Yesterday error counts
...
```

**4. Final Insight**:
```
"ê°€ì¥ ë¬¸ì œê°€ ë§ì€ ì„œë¹„ìŠ¤ëŠ” payment-apiì…ë‹ˆë‹¤ (250ê±´ ì—ëŸ¬).
ì£¼ìš” ì—ëŸ¬ íƒ€ì…ì€ DatabaseConnectionError (60%)ì™€ TimeoutError (30%)ì…ë‹ˆë‹¤.
ì–´ì œ ëŒ€ë¹„ ì˜¤ëŠ˜ ì—ëŸ¬ê°€ 2ë°° ì¦ê°€í–ˆìœ¼ë©°, íŠ¹íˆ ì˜¤ì „ 10-11ì‹œì— ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ì¦‰ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ í¬ê¸°ë¥¼ í™•ì¸í•˜ê³ , í”¼í¬ ì‹œê°„ëŒ€ ìŠ¤ì¼€ì¼ë§ì„ ê³ ë ¤í•˜ì„¸ìš”."
```

**5. Follow-up with Context**:
```
User: "ê·¸ ì„œë¹„ìŠ¤ë“¤ì˜ ê³µí†µì ì€?"
System:
  - context_resolved: "payment-api, user-api, order-api ì„œë¹„ìŠ¤ë“¤ì˜ ê³µí†µì ì€?"
  - Analyzes common patterns
  - All three services share same database
  - All show DatabaseConnectionError
  - Recommends database-level investigation
```

**6. Cache Check**:
```
User: "ì–´ë–¤ ì„œë¹„ìŠ¤ê°€ ê°€ì¥ ë¬¸ì œì¸ê°€?" (exact same initial question)
System:
  - cache_hit=true
  - Instant response with previous results
```

**Verification Checklist**:
- âœ… Optimization â†’ complexity=complex
- âœ… Planning â†’ 4-step decomposition
- âœ… Multi-step execution successful
- âœ… Each step builds on previous
- âœ… Final insight comprehensive
- âœ… Context preserved for follow-up
- âœ… Cache works for repeated questions
- âœ… All features integrated smoothly

---

## ğŸ“Š Test Execution Checklist

Copy this to track progress:

```
FEATURE TESTS (24 tests)
========================

[ ] Feature #1: Query Result Cache
  [ ] 1.1 Cache Miss â†’ Hit
  [ ] 1.2 Cache Invalidation
  [ ] 1.3 TTL Expiration

[ ] Feature #2: Context-Aware Agent
  [ ] 2.1 Focus Tracking
  [ ] 2.2 Reference Resolution
  [ ] 2.3 Conversation History
  [ ] 2.4 New Conversation Reset

[ ] Feature #3: Multi-Step Reasoning
  [ ] 3.1 Simple â†’ Single Step
  [ ] 3.2 Complex â†’ Multi-Step
  [ ] 3.3 Step Progress UI
  [ ] 3.4 Step Failure Handling

[ ] Feature #4: Query Optimization
  [ ] 4.1 Complexity Classification
  [ ] 4.2 Strategy Selection
  [ ] 4.3 Index Suggestions

[ ] Feature #5: Alerting & Monitoring
  [ ] 5.1 Manual Check API
  [ ] 5.2 Alert History API
  [ ] 5.3 Error Rate Spike
  [ ] 5.4 Slow API Detection
  [ ] 5.5 Service Down Detection
  [ ] 5.6 Toast Notification UI
  [ ] 5.7 Alert History Page

[ ] Feature #6: Tool Selection
  [ ] 6.1 SQL Tool (default)
  [ ] 6.2 Grep Tool (patterns)
  [ ] 6.3 Metrics Fallback

INTEGRATION SCENARIOS (3 tests)
================================

[ ] Scenario A: Cache + Context + Multi-Step
  [ ] Turn 1: Initial query sets context
  [ ] Turn 2: Same query hits cache
  [ ] Turn 3: Reference resolved from context
  [ ] Turn 4: Complex question triggers multi-step

[ ] Scenario B: Alert + Context
  [ ] Alert generated and broadcast
  [ ] User asks about alert
  [ ] Context-aware detailed analysis

[ ] Scenario C: Complex Pipeline
  [ ] Optimization identifies complexity
  [ ] Planning decomposes into steps
  [ ] Multi-step executes sequentially
  [ ] Insight synthesizes findings
  [ ] Follow-up uses context
  [ ] Cache works on repeat

TOTAL: 27 test cases
```

---

## ğŸ› Troubleshooting Guide

### Issue: Frontend Won't Build

**Symptoms**:
```
ERROR: failed to solve: failed to compute cache key
```

**Diagnosis**:
```bash
# Test local build first
cd frontend
pnpm install
pnpm run build

# Check for build errors
```

**Fixes**:
- Ensure pnpm-lock.yaml exists and is valid
- Check package.json for syntax errors
- Verify all dependencies are compatible
- Try `pnpm install --no-frozen-lockfile` if lock file is corrupted

---

### Issue: Nginx 404 on All Routes

**Symptoms**:
- http://localhost:3000 returns 404
- No files served

**Diagnosis**:
```bash
# Check if dist files exist in container
docker exec -it log-analysis-frontend sh
ls -la /usr/share/nginx/html

# Expected: index.html, assets/, etc.
```

**Fixes**:
- Verify build stage completed successfully
- Check COPY --from=builder path is correct
- Ensure `pnpm run build` creates `dist/` directory
- Check vite.config.ts build.outDir setting

---

### Issue: API Requests Fail (CORS or 502)

**Symptoms**:
- Frontend loads but API calls fail
- Console shows CORS errors or 502 Bad Gateway

**Diagnosis**:
```bash
# Test nginx proxy from inside container
docker exec -it log-analysis-frontend sh
wget -O- http://log-analysis-server:8000/health

# Should return: {"status": "healthy"}
```

**Fixes**:
- Verify log-analysis-server is running: `docker-compose ps`
- Check service names in nginx.conf match docker-compose.yml
- Ensure both services on same network (log-network)
- Check nginx logs: `docker logs log-analysis-frontend`

---

### Issue: WebSocket Connection Failed

**Symptoms**:
```
WebSocket connection to 'ws://localhost:3000/ws/query' failed
```

**Diagnosis**:
```bash
# Check nginx WebSocket proxy config
docker exec -it log-analysis-frontend cat /etc/nginx/conf.d/default.conf

# Verify Upgrade headers present
grep -A5 "location /ws/" /etc/nginx/conf.d/default.conf
```

**Fixes**:
- Ensure nginx.conf has Upgrade and Connection headers
- Check proxy_read_timeout is long enough (7d recommended)
- Verify proxy_buffering off for real-time streaming
- Test backend WebSocket directly:
  ```bash
  # From host
  wscat -c ws://localhost:8001/ws/query
  ```

---

### Issue: Alerts Not Received in Frontend

**Symptoms**:
- Background task runs but frontend doesn't show alerts
- No toast notifications

**Diagnosis**:
```bash
# Check if background task is running
docker-compose logs log-analysis-server | grep -i "anomaly"

# Check active WebSocket connections
# (Add logging to websocket.py to show connection count)

# Manual alert trigger
curl -X POST http://localhost:8001/alerts/check
```

**Fixes**:
- Verify WebSocket connection is active
- Check active_connections list in websocket.py
- Ensure broadcast_alert function is called
- Verify AlertNotification component is mounted
- Check alertStore subscription

---

### Issue: Multi-Step Progress Not Showing

**Symptoms**:
- Complex queries execute but no progress UI
- MultiStepProgress component not rendered

**Diagnosis**:
```bash
# Check if plan_generated event is emitted
# Browser DevTools â†’ Network â†’ WS â†’ Messages

# Verify is_multi_step flag
```

**Fixes**:
- Check planner.py returns is_multi_step=true for complex queries
- Verify Home.svelte listens for plan_generated event
- Ensure stepStatuses state updates trigger reactivity
- Check MultiStepProgress component import

---

## ğŸ¯ Success Criteria

### Build & Deploy
- âœ… `docker-compose build` completes without errors
- âœ… `docker-compose up -d` starts all 4 services
- âœ… All services pass health checks within 30 seconds
- âœ… Frontend accessible at http://localhost:3000

### Functional Tests
- âœ… At least 24/27 tests pass (>88%)
- âœ… All critical features work (cache, context, multi-step)
- âœ… WebSocket streaming functional
- âœ… No unhandled errors in browser console

### Performance
- âœ… Cache hit: <100ms response time
- âœ… Simple query: <3s total time
- âœ… Multi-step: <15s for 4 steps
- âœ… Frontend load: <2s initial load

### User Experience
- âœ… Can ask questions and receive answers
- âœ… Real-time event streaming visible
- âœ… UI components render correctly
- âœ… No crashes or connection drops
- âœ… Error messages clear and helpful

---

## ğŸ“ˆ Test Metrics Template

**File**: `TEST_RESULTS.md` (create during testing)

```markdown
# Test Execution Results

**Date**: 2026-02-05
**Environment**: Docker Compose (4 services)
**Frontend URL**: http://localhost:3000
**Backend URL**: http://localhost:8001
**Tester**: [Your Name]

## Summary

- **Total Tests**: 27
- **Passed**: __/27 (__%)
- **Failed**: __/27 (__%)
- **Skipped**: __/27 (__%)

## Performance Metrics

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Cache hit time | <100ms | __ ms | âœ…/âŒ |
| Simple query time | <3s | __ s | âœ…/âŒ |
| Multi-step time (4 steps) | <15s | __ s | âœ…/âŒ |
| Frontend load time | <2s | __ s | âœ…/âŒ |
| Alert detection latency | <6min | __ min | âœ…/âŒ |

## Feature Results

### Feature #1: Query Result Cache
- [x] 1.1 Cache Miss â†’ Hit - **PASS** (cache_hit toggles correctly, 85ms on hit)
- [ ] 1.2 Cache Invalidation - **FAIL** (cache not cleared, investigating...)
- [x] 1.3 TTL Expiration - **PASS** (expires after 5 min)

**Notes**: Issue with invalidate_cache endpoint - need to check endpoint registration

### Feature #2: Context-Aware Agent
- [x] 2.1 Focus Tracking - **PASS**
- [x] 2.2 Reference Resolution - **PASS** (90% accuracy)
- [x] 2.3 Conversation History - **PASS**
- [ ] 2.4 New Conversation - **FAIL** (button not found in UI)

**Notes**: Need to verify button was added to Home.svelte

... (continue for all features)

## Issues Found

### Issue #1: WebSocket Reconnection
- **Severity**: Medium
- **Description**: After 3 failed reconnects, WebSocket gives up
- **Impact**: User must refresh page to reconnect
- **Fix**: Increase maxReconnectAttempts to 5-10 or add manual reconnect button

### Issue #2: Alert Toast Z-Index
- **Severity**: Low
- **Description**: Toast notification overlaps with chat input on small screens
- **Impact**: Minor UI annoyance
- **Fix**: Adjust CSS z-index or positioning

... (continue documenting issues)

## Recommendations

1. **Critical**: Fix issue #X (prevents core functionality)
2. **Important**: Address issue #Y (affects UX)
3. **Enhancement**: Consider improvement #Z

## Sign-off

- [ ] All critical issues resolved
- [ ] >90% test pass rate achieved
- [ ] Performance targets met
- [ ] Ready for production deployment

**Tester Signature**: ________________
**Date**: ________________
```

---

## ğŸš€ Quick Test Commands

### Health Checks
```bash
# All services
docker-compose ps

# Individual health
curl http://localhost:8001/health
curl http://localhost:3000
curl http://localhost:8000/docs

# Database
docker exec -it log-analysis-postgres psql -U loguser -d logdb -c "SELECT COUNT(*) FROM logs;"
```

### Log Monitoring
```bash
# All services
docker-compose logs -f

# Specific service
docker-compose logs -f log-analysis-server
docker-compose logs -f frontend

# Filter for errors
docker-compose logs | grep -i error
```

### Test Data Generation
```bash
# Normal baseline
python scripts/generate_test_logs.py --scenario normal --count 100

# Error spike (for alerts)
python scripts/generate_test_logs.py --scenario error_spike --count 100

# Slow APIs
python scripts/generate_test_logs.py --scenario slow_api

# Service down simulation
python scripts/generate_test_logs.py --scenario service_down

# All scenarios
python scripts/generate_test_logs.py --scenario all
```

### Alert Testing
```bash
# Manual anomaly check
curl -X POST http://localhost:8001/alerts/check

# Alert history
curl http://localhost:8001/alerts/history?limit=10

# Cache invalidation
curl -X POST http://localhost:8001/invalidate_cache
```

---

## ğŸ“ Test Best Practices

### Before Testing
1. **Clean State**: Start with fresh containers
   ```bash
   docker-compose down -v  # Remove volumes
   docker-compose up -d
   ```

2. **Baseline Data**: Generate normal logs first
   ```bash
   python scripts/generate_test_logs.py --scenario normal --count 50
   ```

3. **Browser DevTools**: Open Network tab â†’ WS filter
   - Watch WebSocket messages in real-time
   - Verify event types and data

### During Testing
1. **One Feature at a Time**: Complete all tests for one feature before moving to next
2. **Document Everything**: Note exact steps, screenshots, and observations
3. **Reproduce Issues**: If a test fails, run it 2-3 times to confirm
4. **Check Logs**: Always check Docker logs when something fails

### After Testing
1. **Calculate Metrics**: Update TEST_RESULTS.md with actual numbers
2. **Prioritize Issues**: Critical â†’ Important â†’ Nice-to-have
3. **Fix and Retest**: Address issues and re-run failed tests
4. **Final Validation**: Run integration scenarios to ensure fixes didn't break other features

---

## ğŸ“š Additional Resources

### Useful Commands

**Docker Management**:
```bash
# Rebuild specific service
docker-compose build --no-cache frontend

# Restart specific service
docker-compose restart log-analysis-server

# View resource usage
docker stats

# Clean up
docker-compose down
docker system prune -a  # Remove unused images/containers
```

**Database Queries**:
```bash
# Direct SQL access
docker exec -it log-analysis-postgres psql -U loguser -d logdb

# Useful queries
SELECT COUNT(*) FROM logs WHERE deleted = FALSE;
SELECT service, COUNT(*) FROM logs GROUP BY service;
SELECT level, COUNT(*) FROM logs GROUP BY level;
```

**Log Analysis**:
```bash
# Error count in logs
docker-compose logs log-analysis-server | grep -i "error" | wc -l

# WebSocket messages
docker-compose logs log-analysis-server | grep -i "websocket"

# Cache operations
docker-compose logs log-analysis-server | grep -i "cache"
```

---

## âœ… Test Completion Criteria

### Must Achieve
- [x] All 4 Docker services running and healthy
- [x] Frontend accessible and functional
- [x] At least 24/27 tests passing (>88%)
- [x] All critical features operational
- [x] No blocking bugs

### Should Achieve
- [ ] 27/27 tests passing (100%)
- [ ] All performance targets met
- [ ] No errors in Docker logs
- [ ] All edge cases handled

### Nice to Have
- [ ] Performance benchmarks documented
- [ ] Video walkthrough of features
- [ ] README updated with Docker setup
- [ ] CI/CD pipeline for automated testing

---

**Total Test Cases**: 27 (24 feature + 3 integration)

**Estimated Test Time**: 3-4 hours (including wait times for alerts)

**Success Rate Target**: >90% (at least 24 passing)
