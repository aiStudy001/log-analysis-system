# Log Collector - Python Client

ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ë¡œê·¸ ìˆ˜ì§‘ í´ë¼ì´ì–¸íŠ¸ for Python

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

## ğŸ“‹ Prerequisites

Before using this library, ensure you have:

- **Python 3.8+** installed
- **Package manager**: pip
- **Log server running**: See [Log Save Server Setup](../../services/log-save-server/README.md)
- **PostgreSQL database**: For log storage (v12+)
- **Basic async knowledge**: Understanding of threading and queue patterns

## ğŸ¯ Why Use This Library?

### The Problem
Traditional logging blocks your application, creating performance bottlenecks:
- Each log = 1 HTTP request = ~50ms blocked time
- 100 logs/sec = 5 seconds of blocking per second (impossible!)
- Application threads wait for network I/O
- Database connection pool exhaustion

### The Solution
Asynchronous batch logging with zero blocking:
- âœ… **~0.1ms per log** - App never blocks waiting for network
- âœ… **Batches 1000 logs** - Single HTTP request instead of 1000
- âœ… **Background thread** - Separate daemon thread handles transmission
- âœ… **Auto compression** - gzip reduces bandwidth by ~70%
- âœ… **Reliable delivery** - Automatic retries with exponential backoff
- âœ… **Graceful shutdown** - Flushes queue before exit, zero log loss

### When to Use This
- High-traffic applications (>100 requests/sec)
- Performance-critical paths where blocking is unacceptable
- Microservices needing centralized structured logging
- Distributed tracing across services
- PostgreSQL-based log analysis and querying

### When NOT to Use This
- Low-traffic apps (<10 req/sec) - simple file logging is fine
- Quick debugging sessions - use print() for speed
- Need real-time log streaming - use dedicated streaming solutions
- Cannot run log server infrastructure - use cloud logging services

## ğŸš€ Quick Start (30 seconds)

### Step 1: Install
```bash
pip install log-collector-async
```

### Step 2: Use in your app
```python
from log_collector import AsyncLogClient

# Initialize logger
logger = AsyncLogClient("http://localhost:8000")

# Send logs - non-blocking, ~0.1ms
logger.info("Hello world!", user_id="123", action="test")
logger.warn("High memory usage", memory_mb=512)
logger.error("Database error", error="connection timeout")

# Logs are batched and sent automatically every 1 second or 1000 logs
```

### Step 3: Check logs in database
```bash
psql -h localhost -U postgres -d logs_db \
  -c "SELECT * FROM logs ORDER BY created_at DESC LIMIT 5;"
```

**Want more details?** See [Framework Integration](#-feature-2-http-ì»¨í…ìŠ¤íŠ¸-ìë™-ìˆ˜ì§‘) below.

**Want a working example?** Check out [Demo Applications](#-live-demo).

## ğŸ“º Live Demo

See working examples with full context tracking:

### Python + FastAPI
- **Location**: [tests/demo-app/backend-python/](../../tests/demo-app/backend-python/)
- **Features**: Login, CRUD operations, error handling, slow API testing
- **Run**: `python tests/demo-app/backend-python/server.py`

### JavaScript + Express
- **Location**: [tests/demo-app/backend/](../../tests/demo-app/backend/)
- **Features**: Same features but with JavaScript
- **Run**: `node tests/demo-app/backend/server.js`

### Frontend Integration
- **Location**: [tests/demo-app/frontend/](../../tests/demo-app/frontend/)
- **Features**: Browser-based logging with proper CORS setup
- **Run**: Open `tests/demo-app/frontend/index-python.html` in browser

### Quick Demo Setup
```bash
# 1. Start log server (in Docker)
cd services/log-save-server
docker-compose up

# 2. Start backend (Python or JavaScript)
cd tests/demo-app/backend-python
python server.py

# 3. Open frontend
open ../frontend/index-python.html

# 4. Interact with app, then check logs
psql -h localhost -U postgres -d logs_db \
  -c "SELECT service, level, message FROM logs ORDER BY created_at DESC LIMIT 10;"
```

## ğŸ”— Integration with Full System

This client is part of a complete log analysis system. See the [main README](../../README.md) for the full picture.

### System Architecture

```
[Your App] â†’ [Python Client] â†’ [Log Save Server] â†’ [PostgreSQL] â†’ [Analysis Server] â†’ [Frontend]
```

### Related Components

- **Log Save Server**: Receives logs via HTTP POST ([README](../../services/log-save-server/README.md))
- **Log Analysis Server**: Text-to-SQL with Claude Sonnet 4.5 ([README](../../services/log-analysis-server/README.md))
- **Frontend Dashboard**: Svelte 5 web interface ([README](../../frontend/README.md))
- **JavaScript Client**: JavaScript async log collection ([README](../javascript/README.md))
- **Database Schema**: PostgreSQL 15 with 21 fields ([schema.sql](../../database/schema.sql))

### Quick System Setup

For a complete local environment with all components:

```bash
# From root directory
docker-compose up -d
# Starts: PostgreSQL, Log Save Server, Log Analysis Server, Frontend
```

See [QUICKSTART.md](../../QUICKSTART.md) for detailed setup.

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

- âš¡ **ë¹„ë¸”ë¡œí‚¹ ë¡œê¹…** - ì•± ë¸”ë¡œí‚¹ < 0.1ms
- ğŸš€ **ë°°ì¹˜ ì „ì†¡** - 1000ê±´ or 1ì´ˆë§ˆë‹¤ ìë™ ì „ì†¡
- ğŸ“¦ **ìë™ ì••ì¶•** - gzip ì••ì¶•ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ë¹„ìš© ì ˆê°
- ğŸ”„ **Graceful Shutdown** - ì•± ì¢…ë£Œ ì‹œ í ìë™ flush
- ğŸ¯ **ìë™ í•„ë“œ ìˆ˜ì§‘** - í˜¸ì¶œ ìœ„ì¹˜, HTTP ì»¨í…ìŠ¤íŠ¸, ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ìë™ í¬í•¨
- ğŸŒ **ì›¹ í”„ë ˆì„ì›Œí¬ í†µí•©** - Flask, FastAPI, Django ì§€ì›
- ğŸ” **ë¶„ì‚° ì¶”ì ** - trace_idë¡œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ ìš”ì²­ ì¶”ì 

## ğŸ“¦ Installation

```bash
pip install log-collector-async
```

Development dependencies (for testing):
```bash
pip install log-collector-async[dev]
```

## ğŸ’¡ Basic Usage

### Standard Usage

```python
from log_collector import AsyncLogClient

# Initialize with options
logger = AsyncLogClient(
    server_url="http://localhost:8000",
    service="my-service",
    environment="production"
)

# Send logs (non-blocking, batched automatically)
logger.info("Application started")
logger.warn("High memory usage detected", memory_mb=512)
logger.error("Database connection failed", db_host="localhost")

# Automatic graceful shutdown on process exit
```

### Environment Variables

`.env` file or environment variables:
```bash
LOG_SERVER_URL=http://localhost:8000
SERVICE_NAME=payment-api
NODE_ENV=production
SERVICE_VERSION=v1.2.3
LOG_TYPE=BACKEND
```

```python
# Auto-load from environment variables
logger = AsyncLogClient()
```

## ğŸ¯ Feature 1: ìë™ í˜¸ì¶œ ìœ„ì¹˜ ì¶”ì 

**ëª¨ë“  ë¡œê·¸ì— `function_name`, `file_path` ìë™ í¬í•¨!**

```python
def process_payment(amount):
    logger.info("Processing payment", amount=amount)
    # â†’ function_name="process_payment", file_path="/app/payment.py" ìë™ í¬í•¨!

# ë¹„í™œì„±í™”ë„ ê°€ëŠ¥
logger.log("INFO", "Manual log", auto_caller=False)
```

**PostgreSQL ë¶„ì„:**
```sql
SELECT function_name, COUNT(*) as call_count
FROM logs
WHERE created_at > NOW() - INTERVAL '1 hour'
GROUP BY function_name
ORDER BY call_count DESC;
```

## ğŸŒ Feature 2: HTTP ì»¨í…ìŠ¤íŠ¸ ìë™ ìˆ˜ì§‘

**ì›¹ í”„ë ˆì„ì›Œí¬ í™˜ê²½ì—ì„œ `path`, `method`, `ip` ìë™ í¬í•¨!**

### Flask í†µí•©

```python
import time
import uuid
from flask import Flask, request, g
from log_collector import AsyncLogClient

app = Flask(__name__)
logger = AsyncLogClient("http://localhost:8000")

@app.before_request
def setup_log_context():
    """ìš”ì²­ë§ˆë‹¤ ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    # ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ g ê°ì²´ì— ì €ì¥
    g.log_context = {
        'path': request.path,
        'method': request.method,
        'ip': request.remote_addr,
        'trace_id': request.headers.get('x-trace-id', str(uuid.uuid4()).replace('-', '')[:32])
    }

    # ì‚¬ìš©ì IDê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if request.headers.get('x-user-id'):
        g.log_context['user_id'] = request.headers['x-user-id']

    # ìš”ì²­ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    g.start_time = time.time()

    # ìš”ì²­ ì‹œì‘ ë¡œê·¸
    logger.info("Request received", **g.log_context)

@app.after_request
def log_response(response):
    """ì‘ë‹µ ì™„ë£Œ ì‹œ ë¡œê·¸"""
    if hasattr(g, 'log_context') and hasattr(g, 'start_time'):
        duration_ms = int((time.time() - g.start_time) * 1000)
        logger.info("Request completed",
                   status_code=response.status_code,
                   duration_ms=duration_ms,
                   **g.log_context)
    return response

@app.route('/api/users/<user_id>')
def get_user(user_id):
    # ë¼ìš°íŠ¸ í•¸ë“¤ëŸ¬ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì „ë‹¬
    logger.info(f"Fetching user {user_id}",
                user_id_param=user_id,
                **g.log_context)
    # â†’ path, method, ip, trace_id ëª¨ë‘ ìë™ í¬í•¨!
    return {"user_id": user_id}

@app.route('/api/todos', methods=['POST'])
def create_todo():
    logger.info("Creating todo",
                todo_text=request.json.get('text'),
                **g.log_context)
    # ... handle todo creation
    return {"success": True}
```

### FastAPI í†µí•©

```python
import time
import uuid
from fastapi import FastAPI, Request
from log_collector import AsyncLogClient

app = FastAPI()
logger = AsyncLogClient("http://localhost:8000")

@app.middleware("http")
async def log_context_middleware(request: Request, call_next):
    """HTTP ì»¨í…ìŠ¤íŠ¸ ë¯¸ë“¤ì›¨ì–´"""
    # ìš”ì²­ ì‹œì‘ ì‹œê°„
    start_time = time.time()

    # trace_id ìƒì„±
    trace_id = request.headers.get("x-trace-id", str(uuid.uuid4()).replace("-", "")[:32])

    # HTTP ì»¨í…ìŠ¤íŠ¸
    log_context = {
        "path": request.url.path,
        "method": request.method,
        "ip": request.client.host if request.client else None,
        "trace_id": trace_id,
    }

    # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    user_id = request.headers.get("x-user-id")
    if user_id:
        log_context["user_id"] = user_id

    # ìš”ì²­ ì»¨í…ìŠ¤íŠ¸ë¥¼ request.stateì— ì €ì¥
    request.state.log_context = log_context
    request.state.start_time = start_time

    logger.info("Request received", **log_context)

    # ìš”ì²­ ì²˜ë¦¬
    response = await call_next(request)

    # ì‘ë‹µ ì™„ë£Œ
    duration_ms = int((time.time() - start_time) * 1000)
    logger.info("Request completed",
                status_code=response.status_code,
                duration_ms=duration_ms,
                **log_context)

    return response

@app.get("/api/users/{user_id}")
async def get_user(request: Request, user_id: int):
    # ë¼ìš°íŠ¸ í•¸ë“¤ëŸ¬ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì „ë‹¬
    log_ctx = request.state.log_context
    logger.info(f"Fetching user {user_id}",
                user_id_param=user_id,
                **log_ctx)
    # â†’ path, method, ip, trace_id ëª¨ë‘ ìë™ í¬í•¨!
    return {"user_id": user_id}

@app.post("/api/todos")
async def create_todo(request: Request, body: dict):
    log_ctx = request.state.log_context
    logger.info("Creating todo",
                todo_text=body.get('text'),
                **log_ctx)
    # ... handle todo creation
    return {"success": True}
```

## ğŸ‘¤ Feature 3: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

**`user_id`, `trace_id`, `session_id` ë“±ì„ ëª¨ë“  ë¡œê·¸ì— ìë™ í¬í•¨!**

### Context Manager ë°©ì‹ (ê¶Œì¥)

```python
# íŠ¹ì • ë¸”ë¡ì—ë§Œ ì»¨í…ìŠ¤íŠ¸ ì ìš©
with AsyncLogClient.user_context(
    user_id="user_123",
    trace_id="trace_xyz",
    session_id="sess_abc"
):
    logger.info("User logged in")
    # â†’ user_id, trace_id, session_id ìë™ í¬í•¨!

    process_payment()
    logger.info("Payment completed")
    # â†’ í•˜ìœ„ í•¨ìˆ˜ì—ì„œë„ ìë™ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€!

# with ë¸”ë¡ ë²—ì–´ë‚˜ë©´ ìë™ ì´ˆê¸°í™”
```

### ì¤‘ì²© ì»¨í…ìŠ¤íŠ¸ (ìë™ ë³‘í•©)

```python
# ì™¸ë¶€: tenant_id
with AsyncLogClient.user_context(tenant_id="tenant_1"):
    logger.info("Tenant operation")
    # â†’ tenant_id="tenant_1"

    # ë‚´ë¶€: user_id ì¶”ê°€
    with AsyncLogClient.user_context(user_id="user_123"):
        logger.info("User operation")
        # â†’ tenant_id="tenant_1", user_id="user_123" ë‘˜ ë‹¤ í¬í•¨!
```

### ë¶„ì‚° ì¶”ì  (Distributed Tracing)

```python
import uuid

def handle_request():
    trace_id = str(uuid.uuid4())

    with AsyncLogClient.user_context(trace_id=trace_id, user_id="user_123"):
        logger.info("Request received")
        call_service_a()  # Service A í˜¸ì¶œ
        call_service_b()  # Service B í˜¸ì¶œ
        logger.info("Request completed")
        # â†’ ëª¨ë“  ë¡œê·¸ê°€ ê°™ì€ trace_idë¡œ ì¶”ì  ê°€ëŠ¥!
```

**PostgreSQL ë¶„ì„:**
```sql
-- trace_idë¡œ ì „ì²´ ìš”ì²­ íë¦„ ì¶”ì 
SELECT created_at, service, function_name, message, duration_ms
FROM logs
WHERE trace_id = 'your-trace-id'
ORDER BY created_at;
```

### Set/Clear ë°©ì‹

```python
# ë¡œê·¸ì¸ ì‹œ
AsyncLogClient.set_user_context(
    user_id="user_123",
    session_id="sess_abc"
)

logger.info("User action")
# â†’ user_id, session_id ìë™ í¬í•¨

# ë¡œê·¸ì•„ì›ƒ ì‹œ
AsyncLogClient.clear_user_context()
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### íƒ€ì´ë¨¸ ì¸¡ì •

```python
# ìˆ˜ë™ íƒ€ì´ë¨¸
timer = logger.start_timer()
result = expensive_operation()
logger.end_timer(timer, "INFO", "Operation completed")
# â†’ duration_ms ìë™ ê³„ì‚°

# í•¨ìˆ˜ ë˜í¼ (ë™ê¸°/ë¹„ë™ê¸° ìë™ ê°ì§€)
result = logger.measure(lambda: expensive_operation())
```

### ì—ëŸ¬ ì¶”ì 

```python
try:
    risky_operation()
except Exception as e:
    logger.error_with_trace("Operation failed", exception=e)
    # â†’ stack_trace, error_type, function_name, file_path ìë™ í¬í•¨!
```

### ìˆ˜ë™ Flush

```python
# ì¤‘ìš”í•œ ë¡œê·¸ë¥¼ ì¦‰ì‹œ ì „ì†¡
logger.flush()
```

## âš™ï¸ ì„¤ì • ì˜µì…˜

```python
logger = AsyncLogClient(
    server_url="http://localhost:8000",
    service="payment-api",
    environment="production",
    service_version="v1.2.3",
    log_type="BACKEND",
    batch_size=1000,          # ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 1000)
    flush_interval=1.0,       # Flush ê°„ê²© ì´ˆ (ê¸°ë³¸: 1.0)
    enable_compression=True   # gzip ì••ì¶• (ê¸°ë³¸: True)
)
```

## ğŸ“Š ì„±ëŠ¥

- **ì•± ë¸”ë¡œí‚¹**: < 0.1ms per log
- **ì²˜ë¦¬ëŸ‰**: > 10,000 logs/sec
- **ë©”ëª¨ë¦¬**: < 10MB (1000ê±´ í)
- **ì••ì¶•ë¥ **: ~70% (100ê±´ ì´ìƒ ì‹œ ìë™ ì••ì¶•)

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
pytest tests/

# í†µí•© í…ŒìŠ¤íŠ¸ (ë¡œê·¸ ì„œë²„ í•„ìš”)
pytest tests/test_integration.py

# ì»¤ë²„ë¦¬ì§€
pytest --cov=log_collector tests/
```

## ğŸ“ ë¡œê·¸ ë ˆë²¨

```python
logger.trace("Trace message")    # TRACE
logger.debug("Debug message")    # DEBUG
logger.info("Info message")      # INFO
logger.warn("Warning message")   # WARN
logger.error("Error message")    # ERROR
logger.fatal("Fatal message")    # FATAL
```

## ğŸ” PostgreSQL ì¿¼ë¦¬ ì˜ˆì œ

### ì‚¬ìš©ìë³„ ë¡œê·¸ ì¡°íšŒ
```sql
SELECT * FROM logs
WHERE user_id = 'user_123'
ORDER BY created_at DESC
LIMIT 100;
```

### ì—ëŸ¬ ë°œìƒë¥ 
```sql
SELECT
    path,
    method,
    COUNT(*) as total_requests,
    COUNT(CASE WHEN level = 'ERROR' THEN 1 END) as errors,
    ROUND(100.0 * COUNT(CASE WHEN level = 'ERROR' THEN 1 END) / COUNT(*), 2) as error_rate
FROM logs
WHERE created_at > NOW() - INTERVAL '1 hour'
GROUP BY path, method
ORDER BY error_rate DESC;
```

### í•¨ìˆ˜ë³„ ì„±ëŠ¥
```sql
SELECT
    function_name,
    COUNT(*) as calls,
    AVG(duration_ms) as avg_ms,
    MAX(duration_ms) as max_ms
FROM logs
WHERE duration_ms IS NOT NULL
GROUP BY function_name
ORDER BY avg_ms DESC;
```

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **ë¯¼ê°í•œ ì •ë³´ í¬í•¨ ê¸ˆì§€**
   ```python
   # âŒ ì ˆëŒ€ ì•ˆ ë¨!
   logger.info("Login", password="secret")

   # âœ… ì‹ë³„ìë§Œ ì‚¬ìš©
   logger.info("Login successful", user_id="user_123")
   ```

2. **ê³¼ë„í•œ ë¡œê¹… í”¼í•˜ê¸°**
   ```python
   # âŒ ë£¨í”„ ë‚´ë¶€ì—ì„œ ê³¼ë„í•œ ë¡œê¹…
   for i in range(10000):
       logger.debug(f"Processing {i}")

   # âœ… ì£¼ìš” ì´ë²¤íŠ¸ë§Œ ë¡œê¹…
   logger.info(f"Batch processing started", count=10000)
   ```

## ğŸ”§ Troubleshooting

### Logs not appearing in database

**Symptoms**:
- `logger.info()` runs without errors
- No logs visible in PostgreSQL
- No errors in console

**Checklist**:
1. âœ… **Log server running?**
   ```bash
   curl http://localhost:8000/
   # Should return: {"status": "ok"}
   ```

2. âœ… **PostgreSQL running?**
   ```bash
   psql -h localhost -U postgres -d logs_db -c "SELECT 1;"
   ```

3. âœ… **Schema created?**
   ```bash
   psql -h localhost -U postgres -d logs_db -c "\dt"
   # Should show 'logs' table
   ```

4. âœ… **Batch flushed?**
   - Wait 1 second (default flush interval)
   - OR manually flush: `logger.flush()`

5. âœ… **Check server logs**:
   ```bash
   cd services/log-save-server
   docker-compose logs -f
   # Look for "Received X logs" messages
   ```

---

### "Connection refused" errors

**Symptoms**:
```
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionRefusedError(111, 'Connection refused'))
```

**Cause**: Log server not running

**Solution**:
```bash
cd services/log-save-server
docker-compose up -d

# Verify it's running
curl http://localhost:8000/
```

---

### High memory usage

**Symptoms**:
- Application memory grows over time
- Eventually crashes with OOM error

**Cause**: Batch size too large or flush interval too long

**Solution**: Reduce batching parameters
```python
logger = AsyncLogClient(
    "http://localhost:8000",
    batch_size=500,      # Reduce from 1000
    flush_interval=0.5   # Reduce from 1.0
)
```

---

### Logs delayed or not sent on app shutdown

**Symptoms**:
- Last few logs before shutdown are missing
- Queue not flushing properly

**Cause**: App exits before background thread flushes

**Solution**: Call flush before exit
```python
import atexit
import signal

# Auto-flush on normal exit
atexit.register(logger.flush)

# Flush on SIGTERM
def handle_sigterm(signum, frame):
    logger.flush()
    sys.exit(0)

signal.signal(signal.SIGTERM, handle_sigterm)

# Or manually before exit
logger.flush()  # Blocks until queue is empty
```

---

### Thread daemon warnings on exit

**Symptoms**:
```
Exception ignored in: <module 'threading' from '/usr/lib/python3.8/threading.py'>
RuntimeError: can't create new thread at interpreter shutdown
```

**Cause**: Background thread still running during shutdown

**Solution**: Call flush to ensure clean shutdown
```python
# At the end of your application
logger.flush()
```

---

### UnicodeEncodeError with emojis (Windows)

**Symptoms**:
```
UnicodeEncodeError: 'cp949' codec can't encode character
```

**Cause**: Windows console encoding issue

**Solution**: Set UTF-8 encoding
```bash
# Set environment variable before running
set PYTHONIOENCODING=utf-8
python your_app.py

# Or in code
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
```

## ğŸ“‹ Version Compatibility

| Component | Minimum Version | Tested Version | Notes |
|-----------|----------------|----------------|-------|
| **This Client** | 1.0.0 | 1.0.0 | Current release |
| **Log Save Server** | 1.0.0 | 1.0.0 | FastAPI 0.104+ |
| **PostgreSQL** | 12 | 15 | Requires JSONB support |
| **Log Analysis Server** | 1.0.0 | 1.0.0 | Optional (for Text-to-SQL) |
| **Python** | 3.8 | 3.11 | Runtime environment |

### Breaking Changes

- **v1.0.0**: Initial release

### Upgrade Guide

No upgrades yet. This is the initial release.

## ğŸ“š ì¶”ê°€ ë¬¸ì„œ

- [HTTP-CONTEXT-GUIDE.md](HTTP-CONTEXT-GUIDE.md) - HTTP ì»¨í…ìŠ¤íŠ¸ ì™„ì „ ê°€ì´ë“œ
- [USER-CONTEXT-GUIDE.md](USER-CONTEXT-GUIDE.md) - ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì™„ì „ ê°€ì´ë“œ
- [FIELD-AUTO-COLLECTION.md](FIELD-AUTO-COLLECTION.md) - ìë™ í•„ë“œ ìˆ˜ì§‘ ìƒì„¸

## ğŸ¤ ê¸°ì—¬

ê¸°ì—¬ëŠ” ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ ì‚¬ìš©í•˜ì„¸ìš”!

---

**Made with â¤ï¸ by Log Analysis System Team**
